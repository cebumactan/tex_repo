%%%%%%%%%%%%%%%%%%%%%%%%%
%
%
%
% Asymptotic stability of non-autonomous linear system 
%
% contents: 
%
% M-G Lee
%
%
%
%
%
%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[a4paper,11pt]{article}

\usepackage[margin=3cm]{geometry}
\usepackage{setspace}
\onehalfspacing
%\doublespacing
%\usepackage{authblk}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
% \usepackage{calrsfs}
\usepackage[notcite,notref]{showkeys}

\usepackage{psfrag}
\usepackage{graphicx,subfigure}
\usepackage{color}
\def\red{\color{red}}
\def\blue{\color{blue}}
%\usepackage{verbatim}
% \usepackage{alltt}
%\usepackage{kotex}



\usepackage{enumerate}





%%%%%%%%%%%%%% MY DEFINITIONS %%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\upl}{\overline{\lambda}}
\newcommand{\udl}{\underline{\lambda}}
\newcommand{\tl}{{\underline{\theta}}}
\newcommand{\tu}{{\overline{\theta}}}
\newcommand{\bt}{{\bar{t}}}
\newcommand{\E}{\mathcal{E}}

\newcommand{\CC}{\mathcal{C}}


\newcommand{\tcr}{\textcolor{red}}
\newcommand{\tcb}{\textcolor{blue}}
\newcommand{\ubar}[1]{\text{\b{$#1$}}}

\newcounter{Theorem}
\newtheorem{theorem}[Theorem]{Theorem}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{proposition}[Theorem]{Proposition}
\newtheorem{corollary}{Corollary}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{claim}{Claim}
\newtheorem*{theorem*}{Theorem}
\newcounter{mycounter}
\newtheorem{step}{Step}[mycounter]

\theoremstyle{remark}
\newtheorem{remark}{Remark}[section]


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
\title{Asymptotic stability of non-autonomous systems with upper triangular coefficient and a generalization of Levinson's theorem}
\author{Min-Gi Lee\footnotemark[1]}
\date{}

\maketitle
\renewcommand{\thefootnote}{\fnsymbol{footnote}}
\footnotetext[1]{Department of Mathematics, Kyungpook National University,
80 Daehak-ro, Buk-gu, Daegu, 41566, Republic of Korea; leem@knu.ac.kr}
% \footnotetext[2]{Institute of Applied and Computational Mathematics, FORTH, Heraklion, Greece}
% \footnotetext[3]{Department of Mathematics and Applied Mathematics, University of Crete, Heraklion, Greece}
% \footnotetext[4]{Corresponding author : \texttt{athanasios.tzavaras@kaust.edu.sa}}
%\footnotetext[4]{Research supported by the King Abdullah University of Science and Technology (KAUST) }
\renewcommand{\thefootnote}{\arabic{footnote}}

\begin{abstract}
This article is devoted to the study of asymptotic stability of non-autonomous linear systems. The classical theorem of Levinson has been applied to problems wherever time dependent coefficient system arises. However, systems with defective eigenvalues were not covered since the theorem proof relies on diagonalizability. Appealing to block factorization, we give an upper triangular version of Levinson's theorem. Fairly wide class of families of matrices $\{A(t)\}_{t\in \mathbb{R}}$ admits continuous upper triangulation even possibly consists of several such blocks. This points to that the block diagonal version of Levinson's theorem can be applied to wider class of problems with flexibility. We give interpertation of Levinson's theorem and our work as well from perspective of invariant manifold theory.%Working with upper triangular blocks in place of Jordan blocks gives flexibility in applications. 


%We give a upper triangular version of Levinson's theorem. 
%
%
% We stu
% We study the asymptotic stability of a non-autonomous linear system with the time dependent coefficient matrices $\{A(t)\}_{t\in \mathbb{R}}$. The classical theorem of Levinson has been an indispensable tool in studying the asymptotic stability of a non-autonomous linear system. Contrary to the constant coefficient system, having all eigenvalues in the left half complex plane does not imply the asymptotic stability. Levinson's theorem assumes the coefficient matrix is a a suitable perturbation of the diagonal matrix. Our objective is to have a theorem similar to the Levinson's theorem when the family of matrices merely admits the upper triangular factorization. If any eigenvalue is defective, this spoils the prerequisite of the Levinson's theorem, the diagonalization. In turn, nothing is concluded, which is not sharp. In our study, we first reveal the asymptotic behaviors of upper triangular system and use the fixed point theory to come to the conclusion. Unless otherwise we want the asymptotic behavior dimension by dimension, working with upper triangular with internal blocks adds flexibility in the analysis.
\end{abstract}

% \tableofcontents
% \pagebreak


%\vfil\eject


\section{Introduction} \label{sec:intro}
This article is devoted to the study of asymptotic stability for
\begin{equation}
x'(t) = A(t)x(t), \label{eq:0}
\end{equation}
with $x(t) \in \mathbb{R}^N$, $A(t)\in \mathbb{R}^{N\times N}$ for each $t\in \mathbb{R}$. In particular, we study when $A(t)=U(t) + \E(t)$ for $U(t)$ upper triangular and $\E(t)$ suitably small. 


%To ensure the unique existence of the problem on the whole line $-\infty<t<\infty$ and the continuity of the eigenvalues $\lambda_i(t)$, $i=1,\cdots,N$ of $A(t)$, our problem always assumes entries $U_{ij}, \E_{ij}\in C_b( \mathbb{R})$ for $i,j=1,\cdots,N$.
% \begin{equation} \label{A0} \tag{$A0$}
% \begin{aligned}
% &~\text{The entries $\{A_{ij}(t)\}_{t\in \mathbb{R}} \in C_b( \mathbb{R})$}%\\
% % &1.~\text{The entries $\{A_{ij}(t)\}_{t\in \mathbb{R}}$, $i,j=1,\cdots,N$ are continuous at all $t\in \mathbb{R}$,}\\
% % &2.~\text{$\sup_{i,j,t}|A_{ij}(t)| < K$ for a constant $K>0$.}
% \end{aligned}
% \end{equation}

Theorem \ref{thm:3} is a version of generalization of the classical Levinson's theorem \cite{L48}, which has been widely used to study asymptotic stability of {\it non-autonomous systems.} Levinson's theorem applies to a class of problems with coefficient suitably diagonalizable and time dependent. The theorem can be found in many places \cite{BL15,CL55,L19}, and for the readerâ€™s convenience we include the Levinson theorem in the Appendix. The theorem has been applied to tremendously many science and engineering non-autonomous systems. %{\blue linear stabiliy?}%seeks the linear stability of a certain dynamical systems problem. %particular solution of the corresponding dynamical system, espescially when the linearized system has time dependent coefficients. % with the spectral information of the coefficient matrix $A(t)$ given. 



To illustration motivation for generalization, consider	the following simple example,
\begin{equation}\label{eq:ex}
\begin{pmatrix} y_1(t) \\ y_2(t) \\ y_3(t) \end{pmatrix}' = \begin{pmatrix} -1 + \frac{1}{t} & 1 & 0 \\0 & -1- \frac{1}{t} & 0 \\ 0 & 0 & -2\end{pmatrix} \begin{pmatrix} y_1(t) \\ y_2(t) \\ y_3(t) \end{pmatrix}, \quad \begin{pmatrix} -1 + \frac{1}{t} & 1 & 0 \\0 & -1- \frac{1}{t} & 0 \\ 0 & 0 & -2\end{pmatrix}=:U(t).
\end{equation}
By integrating the system, we see that the trivial solution is asymptotically stable. One is asked if this stability can be persistently continued under suitable perturbations. Consider
$$ x'(t) = U(t)x(t) + \E(t)x(t)$$
with $\E(t)$ a perturbation whose smallness will be stated in the sequel. For this example, the three eigenvalues of $U(t)$ are distinct for every finite $t$, hence $U(t)$ is (continuously) diagonalizable  for every  finite $t$. However, It fails to be so in the limit $t \rightarrow \infty$. Because of this failure at infinity, Levinson's theorem for this case is an empty statement; i.e., no result regarding asymptotic stability is provided by the theorem. As a matter of fact, the trivial solution is asymptotically stable with perturbations satisfying the smallness conditions for Levinson's theorem. Thus, there are grounds to generalize the theorem to provide the same conclusions. In particular, to use upper triangular factorization in place of diagonalization is our main concern, allowing defective eigenvalues.



A few previous studies have considered approach for systems with defective eigenvalues, appealing to block diagonalization. Alternate systems with a single Jordan block have been considered \cite{E89} and subsequently extended to systems comprising several Jordan blocks \cite{DK72,BL15}. Systems with multiple Jordan blocks have also beend considered under weak dichotomy assumtions \cite{CK70, BL10}. Section~\ref{sec:discuss}  discusses how these previous studies relate to the current work, and we provide the invariant subspace point of view on those results.


%  As a natural replacement, a system with Jordan block has been studied. Problems with a single Jordan block was treated by \cite{E89}. Later, \cite{DK72} and \cite{BL15} studied the cases when system consists of several Jordan blocks. \cite{CK70} and \cite{BL10} also studied the system with multiple Jordan blocks under a different set of assumptions, which \cite{BL10} called the `weak dichotomy'. The works of those authors in relation to ours is illustrated in Section 4 in which we also present the invariant subspace point of view on those results.

This paper is a completion of the following theorem by the author in \cite[Theorem 1]{L19}.
\begin{theorem*}[\cite{L19}] \label{thm:2} Suppose that $U(t)=diag(U_0(t),U_1(t))$ with $U_0(t)$ and $U_1(t)$ upper triangular and of dimensions $N_0\times N_0$ and $N_1 \times N_1$ respectively. We make the followiing assumptions.
\begin{enumerate}
\item $\int_{a_0}^\infty \|\E(t)\| \; < \infty$ for some $a_0$.
\item There is a real-valued function $\theta$ and constants $\delta >0$ and $A\in \mathbb{R}$ such that
\begin{enumerate}
\item for any $t_2\ge t_1$ and any $\lambda_{0,i}(t)$ $i=1,\cdots,N_0$ of eigenvalues of $U_0(t)$ 
$$ \int_{t_1}^{t_2} Re\: \lambda_{0,i}(t) - \theta(t) \; dt \le A -(N_0 -1 +\delta)\log(1+t_2-t_1),$$
\item and for any $t_2\ge t_1$ and any $\lambda_{1,i}(t)$ $i=1,\cdots,N_1$ of eigenvalues of $U_1(t)$
$$ \int_{t_1}^{t_2} \theta(t)- Re\: \lambda_{1,i}(t) \; dt \le A -(N_1 -1 +\delta)\log(1+t_2-t_1).$$
\end{enumerate}
\end{enumerate}
Then there is a constant $a$ and an $N_0$-dimensional subspace $E_0$ of $ \mathbb{R}^N$ such that $x(a)\in E_0$ implies that $\displaystyle \lim_{t \rightarrow \infty}|x(t)|_\theta =0$. %One such an $E_0 = P_0\mathbb{R}^N$.
\end{theorem*}
In that exposition, a system with two blocks $U(t) = diag\big(U_0(t), U_1(t)\big)$ was considered, with spectral gap between them. We showed only the stability result, i.e., the persistent existence of one part with smaller eigenvalue (in real parts) against the other part, whereas Levinson's theorem states the persistent existence of an orbig with intermediate eigenvalue. Levinson's theorem proof resembles that for center manifold theory, rather than stable manifold theory. Our objective is to complete the approach \cite[Theorem1]{L19}, considering block diagonal systems with a block that spectrally intervenes. 

The remainder of this paper is organized as follows. Section \ref{sec:prelim} provides some key features for non-autonomous system asymptotic stability problems (readers already familiar with those  aspects may prefer to go directly to Section \ref{sec:MainRes}). Section \ref{sec:MainRes} presents the Theorem \ref{thm:3}, the main outcome from the paper. Section 4 presents the invariant manifold point of view on this problem to clarify our aim to do and how this relates to earlier work.

\section{Preliminaries} \label{sec:prelim}
This section details our notations and introduce useful concepts from ordinary differential equation theories required to state the particular problem considered. Readers familiar with those basic notions may prefer to go directly to Section \ref{sec:MainRes}. Much of the detail in this section was from \cite{L19}. 

\subsection{Key observations asymptotic stability of non-autonomous systems}
We use the following example from  \cite{MY60} to illustrate to non-autonomous system asymptotic stability relevance,
\begin{align*}
\begin{pmatrix} x\\y \end{pmatrix}' = \left\{\begin{pmatrix} -\frac{1}{4} & 1\\-1 & -\frac{1}{4} \end{pmatrix}+ \frac{3}{4}\begin{pmatrix} \cos 2t & \sin 2t\\ \sin 2t & -\cos 2t \end{pmatrix}\right\} \begin{pmatrix} x\\y \end{pmatrix}.
\end{align*}
Althouth the time dependent coefficient matrix has eigenvalues $-\frac{1}{4} \pm \frac{\sqrt{7}}{4}i$ for all $t$, and hence the real part is always strictly negative, $\begin{pmatrix} -\cos t\\ \sin t \end{pmatrix}\exp(t/2)$ solves the equation. Thus, there is some deficiency in determining growth behavior frome spectral information alone.

% Apart from full generality, it is possible to deduce asymptotic growth rate estimates from spectrum for systems having coefficient in diagonal or Jordan form. where the system is decoupled in some sense. A perturbation theory having coefficient $U(t) + \E(t)$ with $U(t)$ diagonal or Jordan and $\E(t)$ suitably small was systematically studied by Levinson and other authors. Such a form $U(t)+\E(t)$ can be understood to appear by following procedure. Assuming that a family of matrices $\{A(t)\}_{t\in \mathbb{R}}$ admits a simultaneous factorization $A(t)=S(t)U(t)S(t)^{-1}$ $\forall t$, the change of variable $y(t) = S(t)^{-1}x(t)$
% leads to the system
% \begin{equation*} 
% \begin{aligned}
% y'(t) = U(t)y(t) - S(t)^{-1}S'(t)y(t)
% \end{aligned}
% \end{equation*}
% and we let $\E(t):=-S(t)^{-1}S'(t)$. Its smallness is yet to verify.

Another obvious but important observation is that the growth rate gap, derived from the spectral gap, is finer than that for constant coefficient systems. This can be illustrated by comparing systems
\begin{align*}
\begin{pmatrix} x_1\\y_1 \end{pmatrix}' = \begin{pmatrix} \frac{a}{t} & 0\\0 & \frac{b}{t} \end{pmatrix}\begin{pmatrix} x_1\\y_1 \end{pmatrix}, 
\quad {\text and} \quad 
\begin{pmatrix} x_2\\y_2 \end{pmatrix}' = \begin{pmatrix} a & 0\\0 & b \end{pmatrix}\begin{pmatrix} x_2\\y_2 \end{pmatrix},
\quad a \ne b,
\end{align*}
where the rate gap between independent solutions is $t^{a-b}$ and $e^{(a-b)t}$, respectively. Consequently, adding perturbations of size $\mathcal{O}(\tfrac{1}{t})$ terms as $t \rightarrow \infty$ does spoil  non-autonomous system asymptotic behavior. Levinson's theorem states that such a polynomial growth gap is kept under perturbations of integrable size.

%What is also straightforward is that perturbations of size $o(\tfrac{1}{t})$ as $t \rightarrow \infty$ is negligible in rate calculation since $\int_a^\infty o(\tfrac{1}{t}) \;dt$ is bounded. 

\subsection{Fundamental matrices}


From the Picard-Lindel\"{o}f Theorem, a linear system has a unique solution for $|t-t_0| \le \ell$ with $\ell=\min(a,b/M)$, where $a$, $b$, and $M$ are such that in the domain $|t-t_0|\le a$ and $|x-x_0| \le b$, $f(t,x)$ is continuous in $t$ and is uniformly Lipschitz in $y$ and $|f(t,x)|$ is bounded by $M$. 


For \eqref{eq:0}, we require
% \begin{enumerate}
% \item The entries $\{A_{ij}(t)\}_{t\in \mathbb{R}}$, $i,j=1,\cdots,N$ are continuous at all $t\in \mathbb{R}$.
% \item $|A_{ij}(t)|$ is uniformly bounded by a constant $K>0$.
% \end{enumerate}
\begin{equation} \label{A0} \tag{$A0$}
\begin{aligned}
&1.~\text{Entries $\{A_{ij}(t)\}_{t\in \mathbb{R}}$, $i,j=1,\cdots,N$ are continuous at all $t\in \mathbb{R}$.}\\
&2.~\text{$|A_{ij}(t)|$ is uniformly bounded by a constant $K>0$.}
\end{aligned}
\end{equation}
% the entries $\{A_{ij}(t)\}_{t\in \mathbb{R}}$, $i,j=1,\cdots,N$ are continuous at all $t\in \mathbb{R}$ and is uniformly bounded by a constant $K>0$. 
We will call this assumption $(A0)$, and hence from the Picard-Lindel\"{o}f theorem the solution extends to all $ \mathbb{R}$ uniquely (one could have considered a smooth cut-off if necessary). 

Therefore, for any two numbers $t$ and $\tau$ it makes senses to consider the solution matrix $\Phi(t,\tau)$, which maps $x(\tau)$ to $x(t)$ and the corresponding family $\{\Phi(t,\tau)\}_{t,\tau \in \mathbb{R}}$. The solution matrices may be expressed as $\Phi(t-\tau)$, but  they explicitly depend on $t$ and $\tau$ for a non-autonomous system.

We have that $\Phi(t,t)=\mathbf{1}$ for all $t$ and 
$$\Phi(a,b)\Phi(b,c) = \Phi(a,c), \quad \forall a,b,c.$$
In particular, $\Phi(a,b)$ is always invertible with inverse $\Phi(b,a)$. %They are direct consequeces of unique existence.

\subsection{Operations on block diagonal matrices}

Let $N_1,N_2, \cdots, N_k$ be fixed positive integers such that $\displaystyle\sum_1^k N_\alpha = N$. Consider a collection $\CC$ of all block diagonal $N\times N$ matrices of the form $U = diag(B_1,B_2, \cdots,B_k)$, with blocks of dimensions $N_\alpha\times N_\alpha$. $\CC$ is closed under the matrix multiplication. We find that for $U = diag(B_1,B_2, \cdots,B_k)$ and $W = diag(C_1,C_2, \cdots,C_k)$, $UW = diag(B_1C_1, B_2C_2,\cdots,B_kC_k)$. %, i.e., the multiplication carries out block-wisely.


Let $P_\alpha=diag(\mathbf{0},\cdots,\mathbf{0},\mathbf{1}_{N_\alpha},\mathbf{0},\cdots,\mathbf{0})$ whose only nontrivial block is at the $\alpha$-th site that is the $N_\alpha$-dimensional identity matrix; if $x\in \mathbb{R}^N$, $x_\alpha$ refers to the $N$-dimensional vector $P_\alpha x$; and if $U\in \CC$, $U_\alpha$ refers to the $N\times N$ matrix $P_\alpha U$. It is directly verified that 
$$P_\alpha (UW) = (P_\alpha U) (P_\alpha W), \quad P_\alpha (U x) = (P_\alpha U) (P_\alpha x)$$
and it follows that $P_\alpha(U_1U_2\cdots U_j x) = U_{1\alpha}U_{2\alpha}\cdots U_{j\alpha} x_\alpha.$

\subsection{Notation} \label{sec:notation}
For a vector $x \in \mathbb{R}^N$, $|x|:=\displaystyle\max_{i=1,\cdots,N} |x_i|$. If $A$ is a $N\times N$ matrix, $\|A\|$ denotes the operator norm with respect to the vector norm, i.e., $\|A\|:= \displaystyle\max_{|x|\ne 1} \frac{ |Ax|}{|x|}$. If $x(t)$ is an orbit, then the primary norm is the sup norm $\|x\|_{L^\infty}$. It is convenient to use the weighted norm to compensate the growth appropriately. For  a given real-valued function $\theta$, $$\displaystyle\|x\|_{L^\infty_\theta([a,\infty))} := \sup_{t\ge a} \left|x(t) \exp\left( -\int_a^t \theta(\eta) d\eta\right)\right|$$ or $\|x\|_\theta$ for shortly if there is no confusion about $a$. In that case, $|x(t)|_\theta := \left|x(t) \exp\left( -\int_a^t \theta(\eta) d\eta\right)\right|$ is the weighted length at time $t$. 

For  a family of $N\times N$ upper triangular matrices $\{U(t)\}_{t\in \mathbb{R}}$, let $\lambda_i(t)$, $i=1,\cdots,N$ be the diagonal entries of $U(t)$, which are the eigenvalues of $U(t)$. We also define $\displaystyle\upl(t)\triangleq \max_{i=1,\cdots,N} Re \lambda_i(t)$ and $\displaystyle \udl(t)\triangleq \min_{i=1,\cdots,N} Re \lambda_i(t)$. 


\section{Main Results} \label{sec:MainRes}
Suppose $\{U(t)\}_{t \in \mathbb{R}}$ is a family of upper triangular matrices satisfying $(A0)$. In this section, $y$ solves the system 
\begin{equation}\label{eq:1}
y'(t)=U(t)y(t),
\end{equation}
which we call the upper triangular system and $x$ solves the perturbed system
\begin{equation}\label{eq:2}
x'(t) = U(t)x(t) + \E(t)x(t)
\end{equation}
We denote the family of solution matrices for \eqref{eq:1} will be denoted as $\{\Phi(t,\tau)\}_{t,\tau \in \mathbb{R}}$. To study asymptotic stability for \eqref{eq:2}, we need to know that for \eqref{eq:1}, or estimates on $\|\Phi(t,\tau)\|$. These estimates were calculated previously in terms of spectral information \cite{L19} and we quote the result below.
\begin{proposition}[\cite{L19}]\label{stability} Let $\{U(t)\}_{t \in \mathbb{R}}$ be a given family of upper triangular matrices in \eqref{eq:1} satisfying $(A0)$ and $\{\Phi(t,\tau)\}_{t,\tau \in \mathbb{R}}$ be the corresponding fundamental matrices. Then there is a constant $C_{N,K}>0$ such that for any $a \le b$ and any vector $V \in \mathbb{R}^N$,
\begin{equation} \label{stabestim}
\begin{aligned}
\frac{e^{\int_a^b \udl(\eta) d\eta}}{C_{N,K}(1+b-a)^{N-1}}|V| \le |\Phi(b,a)V|
\le C_{N,K}(1+b-a)^{N-1}e^{\int_a^b \upl(\eta) d\eta}|V|, 
\end{aligned}
\end{equation}
where the constant $C_{N,K}$ depends only on $N$ and $K$. 
\end{proposition}

Now, we consider asymptotic stability for the perturbed system \eqref{eq:2}. The exposition can be via the invariant subspace point of view. Let $U(t)=diag\big(U_0(t),U_1(t)\big)$ with two upper triangular blocks. and split the phase space into $\mathbb{R}^N=E_0\oplus E_1$ for invariant subspaces $E_0$ and $E_1$ corresponding to the respective blocks. Appending the dummy equation $t'=1$ to \eqref{eq:1}, the stability is about the invariant line $M:=\text{$\{t$-axis$\}$}$ and the splitting lifts to that of bundle along $M$. From Proposition \ref{stability}, the range of growth rates in $ E_j$, $j=1,2$ can be estimated by their respective eigenvalues. If two ranges have sufficient gap as $t \rightarrow \infty$, then segregation of the slower subspace persists under perturbations (see \cite[Theorem 1]{L19}).

As discussed in \cite{L19}, the result was only the first half of what would be a parallel statement to Levinson's theorem: Consider a diagonal system with eigenvalues $\lambda_j(t)$, $j=1,\cdots,N$ in ascending order.  Select a $\lambda_k(t)$ that is intermediate. For a diagonal system, the splitting is $\mathbb{R}^N= E_1 \oplus E_2 \oplus \cdots \oplus E_N$ of one dimensional invariant subspaces $E_j := Span~ \mathbf{e}_j$ of coordinate basis. Levinson's theorem can single out the $E_k$ that spectrally intervenes.

Replacing a diagonal matrix by a block diagonal matrix and the one dimensional $E_j$ by those subspaces corresponding to blocks, the result from \cite[Theorem 1]{L19} corresponds to a persistent segregation  $E^s:=E_1 \oplus E_2 \oplus \cdots \oplus E_k$ as a whole. Thus, Theorem \ref{thm:3} complements the remaining half of the persistence theory. Analogously to Levinson's theorem, we can single out $E_k$ out of $E^s$. %$E_1 \oplus E_2 \oplus \cdots \oplus E_m$.


%
%The problem essentially reduces to a problem with three blocks $U(t)=diag\big(U_0(t),U_1(t),U_2(t)\big)$ where the ranges of growth rates are in ascending order. Let the phase space splitting be $\mathbb{R}^N=E_1\oplus E_2 \oplus E_3$ of corresponding invariant subspaces. To segregate $E_1$ that intervines, we first segregate $E_0\oplus E_1$ out of $\mathbb{R}^N$ by Theorem 1 in \cite{L19}. What we will show is that we can segregate $E_1$ out of $E_0\oplus E_1$. The statement in the below is written with $U(t)$ 
\begin{theorem} \label{thm:3} Let $U(t)=diag(U_0(t),U_1(t),\cdots,U_k(t))$ and for $\alpha=1,\cdots,m$ $U_\alpha(t)$ is upper triangular with dimension $N_\alpha\times N_\alpha$. We make the following assumptions% Suppose we can find constants $a$, $\delta>0$, and a real-valued function $\theta$ by which the followings are true.%Let $\displaystyle \udl_1(t)\triangleq \min_{i=1,\cdots,N_1} Re \lambda_{1,i}(t)$ for $U_1(t)$ and $\displaystyle \upl_0(t)\triangleq \max_{i=1,\cdots,N_2} Re \lambda_{0,i}(t)$ for $U_2(t)$ as before.
\begin{enumerate}
\item $\int_{a_0}^\infty \|\E(t)\| \; < \infty$ for some $a_0$.
\item There are real-valued functions $\tu$ and $\tl$ and constants $\delta >0$ and $A\in \mathbb{R}$ by which the followings hold.
\begin{enumerate}
\item $\{1,\cdots,m\}=J_0 \cup J_1 \cup J_2$ and $J_0$, $J_1$, $J_2$ are mutually disjoint. %, where $J_1$ is the singleton $\{\beta\}$.
\item $\alpha \in J_0$ implies that for any $t_2\ge t_1$ and any $i$, $i=1,\cdots,N_\alpha$ 
$$ \int_{t_1}^{t_2} Re\: \lambda_{\alpha,i}(t) - \tl(t) \; dt \le A -(N_\alpha -1 +\delta)\log(1+t_2-t_1),$$
where $\lambda_{\alpha,i}(t) = U_{\alpha,ii}(t)$ of $U_\alpha(t)$.
\item $\alpha \in J_1$ implies that for any $t_2\ge t_1$ and any $i$, $i=1,\cdots,N_\alpha$ 
\begin{align*}
\int_{t_1}^{t_2} Re\: \lambda_{\alpha,i}(t) - \tu(t) \; dt \le A -(N_\alpha -1 +\delta)\log(1+t_2-t_1),\\
\int_{t_1}^{t_2} \tl(t)- Re\: \lambda_{\alpha,i}(t) \; dt \le A -(N_\alpha -1 +\delta)\log(1+t_2-t_1),
\end{align*}
where $\lambda_{\alpha,i}(t) = U_{\alpha,ii}(t)$ of $U_\alpha(t)$.
\item $\alpha \in J_2$ implies that for any $t_2\ge t_1$ and any $i$, $i=1,\cdots,N_\alpha$
$$ \int_{t_1}^{t_2} \tu(t)- Re\: \lambda_{\alpha,i}(t) \; dt \le A -(N_\alpha -1 +\delta)\log(1+t_2-t_1),$$
where $\lambda_{\alpha,i}(t) = U_{\alpha,ii}(t)$ of $U_\alpha(t)$.
\end{enumerate}
\end{enumerate}
Let $\displaystyle N_1 = \sum_{J_1} N_\alpha$. Then there is a constant $a$ and an $N_1$-dimensional subspace $E$ of $\mathbb{R}^N$ such that $x(a) \in E$ implies that $\displaystyle \lim_{t \rightarrow \infty} |x(t)|_\tu =0$ and $\displaystyle \limsup_{t \rightarrow \infty}|x(t)|_\tl = \infty$.
\end{theorem}

\begin{proof}
We first show that the assumptions imply estimates on fundamental matrices: %We use the notations in Section \ref{sec:notation}, $\Phi_\alpha(t,\tau)=P_\alpha \Phi(t,\tau)$ for $\alpha=1,\cdots,k$, where $\{\Phi(t,\tau)\}_{t,\tau \in \mathbb{R}}$ are fundamental matrices of $U(t)$. 
\begin{enumerate}
\item For each $\alpha \in J_0$ and $s\ge \tau$ from Proposition \ref{stability} and our assumptions. 
\begin{align}
\left|\Phi_\alpha(s,\tau)e^{-\int_\tau^s \tl(\eta) \;d\eta}y(\tau)\right| &\le C_1(1 + s-\tau)^{N_\alpha-1}\exp\left(\int_\tau^s \upl_\alpha(\eta)-\tl(\eta) \;d\eta\right)|y(\tau)| \nonumber\\
&\le C_2(1 + s-\tau)^{-\delta}|y(\tau)| \label{eq:decay1}
\end{align}
and for each $\alpha \in J_1\cup J_2$ and $s\le \tau$,
\begin{align}
\left|\Phi_\alpha(s,\tau)e^{\int_s^\tau \tl(\eta)\;d\eta}y(\tau)\right| &\le C_3(1 + \tau -s)^{N_\alpha-1}\exp\left(\int_\tau^s -\udl_\alpha(\eta)+\tl(\eta) \; d\eta\right)|y(\tau)| \nonumber\\
&\le C_4(1 + \tau-s)^{-\delta}|y(\tau)|. \label{eq:decay2}
\end{align}
\item For each $\alpha \in J_0\cup J_1$ and $s\ge \tau$,
\begin{align}
\left|\Phi_\alpha(s,\tau)e^{-\int_\tau^s \tu(\eta) \;d\eta}y(\tau)\right| &\le C_5(1 + s-\tau)^{N_\alpha-1}\exp\left(\int_\tau^s \upl_\alpha(\eta)-\tu(\eta) \;d\eta\right)|y(\tau)| \nonumber\\
&\le C_6(1 + s-\tau)^{-\delta}|y(\tau)| \label{eq:decay3}
\end{align}
and for each $\alpha \in J_2$ and $s\le \tau$,
\begin{align}
\left|\Phi_\alpha(s,\tau)e^{\int_s^\tau \tu(\eta)\;d\eta}y(\tau)\right| &\le C_7(1 + \tau -s)^{N_\alpha-1}\exp\left(\int_\tau^s -\udl_\alpha(\eta)+\tu(\eta) \; d\eta\right)|y(\tau)| \nonumber\\
&\le C_8(1 + \tau-s)^{-\delta}|y(\tau)|. \label{eq:decay4}
\end{align}
\end{enumerate}
We set $\Psi_j(t,\tau):=\displaystyle \sum_{\alpha\in J_j} \Phi_\alpha(t,\tau)$, $Q_j:= \displaystyle \sum_{\alpha\in J_j} P_\alpha$ for $j=0,1,2$, and $C:=\max_{k} {C_k}$.

{\blue
Since $U(t)=diag\big((Q_0+Q_1)U(t), Q_2U(t)\big)$ for two blocks, from estimates \eqref{eq:decay3}-\eqref{eq:decay4}, there exists $a\ge a_0$ such that the integral equation
\begin{equation} \label{integral30}
\left\{ \begin{aligned}
x_0(t) &= \int_a^t \Psi_0(t,\tau) (\E(\tau)x(\tau))_0 \; d\tau,\\
x_1(t) &= y_1(t) + \int_a^t \Psi_1(t,\tau) (\E(\tau)x(\tau))_1 \; d\tau,\\
x_2(t) &= -\int_{t}^\infty \Psi_2(t,\tau) (\E(\tau)x(\tau))_2 \; d\tau.
\end{aligned}\right.
\end{equation}
has unique solution in $L^\infty([a,\infty))$ satisfying $|x|_\tu \rightarrow 0$ as $t \rightarrow \infty$ by Theorem \ref{result0}. Here, $y_1(a)$ is a vector such that $y_1(a)=Q_1y_1(a) = (Q_0+Q_1)y_1(a)$, $y_1(t) = \Psi_1(t,a)y_1(a)$, and $a$ can be chosen so that $C\int_a^\infty \|\E(\tau)\| \;d\tau < \frac{1}{2}$. {\blue dimensions}
}


We claim that $|x(t)|_\tl$ has lower bound away from $0$ for all time $t\ge a$. Pick any $\bar{t} \ge a$ then for any $s \in [a,\bar{t}]$, $x$ also solves the following integral equations in the interval $[a_0,\bt]$.
\begin{equation} \label{integral3}
\left\{ \begin{aligned}
x_0(s) &= \int_a^s \Phi_0(s,\tau) (\E(\tau)x(\tau))_0 \; d\tau,\\
x_1(s) &= \Phi_1(s,\bar{t})x_1(\bar{t}) - \int_s^{\bar{t}} \Phi_1(s,\tau) (\E(\tau)x(\tau))_1 \; d\tau,\\
x_2(s) &= \Phi_2(s,\bar{t})x_2(\bar{t}) - \int_s^{\bar{t}} \Phi_2(s,\tau) (\E(\tau)x(\tau))_2 \; d\tau.
\end{aligned}\right.
\end{equation}
We obtained $\eqref{integral3}_2$  by multiplying $\eqref{integral30}_2$  by $\Phi_1(s,t)$ and substituting $\Phi_1(s,t)y_1(t) = y_1(s) = x_1(s) - \int_a^t \Phi_1(s,\tau)\E(\tau) x(\tau) \; d\tau$. Equation $\eqref{integral3}_3$ was obtained similarly. Define $w_1(s):=\Phi_1(s,\bar{t})x_1(\bar{t})$, $w_2(s):=\Phi_2(s,\bar{t})x_2(\bar{t})$, and $w(s) := w_1(s)+w_2(s)$. Multiplying both sides by $e^{\int_a^s -\tl(\eta) \; d\eta}$,
\begin{equation} \label{integral4}
\left\{ \begin{aligned}
x_0(s)e^{\int_a^s -\tl(\eta) \; d\eta} &= \int_a^s \big(\Phi_0(s,\tau)e^{\int_\tau^s -\tl(\eta) \; d\eta}\big) (\E(\tau)x(\tau)e^{\int_a^\tau -\tl(\eta) \; d\eta})_0 \; d\tau,\\
x_1(s)e^{\int_a^s -\tl(\eta) \; d\eta} &= w_1(s)e^{\int_a^s -\tl(\eta) \; d\eta} \\
&- \int_s^{\bar{t}} \big(\Phi_1(s,\tau)e^{\int_s^\tau \tl(\eta) \; d\eta}\big) (\E(\tau)x(\tau)e^{\int_a^\tau -\tl(\eta) \; d\eta})_1 \; d\tau,\\
x_2(s)e^{\int_a^s -\tl(\eta) \; d\eta} &= w_2(s)e^{\int_a^s -\tl(\eta) \; d\eta}\\
&- \int_s^{\bar{t}} \big(\Phi_2(s,\tau)e^{\int_s^\tau \tl(\eta) \; d\eta}\big) (\E(\tau)x(\tau)e^{\int_a^\tau -\tl(\eta) \; d\eta})_2 \; d\tau.
\end{aligned}\right.
\end{equation}
From estimates \eqref{eq:decay1} and \eqref{eq:decay2},
$$|x(s)-w(s)|_\tl \le \frac{1}{2} \|x\|_\tl \quad \text{and thus } \quad \|x\|_\tl \le 2\|w\|_\tl, \quad \text{where $\|\cdot\|_\tl = \|\cdot\|_{L^\infty_\tl ([a,\bt])}$}.$$
On the other hand, from \eqref{eq:decay2}
\begin{align*}
\|w\|_\tl = \sup_{a\le s\le \bt} \left| \Phi_1(s,\bt)w_1(\bt) + \Phi_2(s,\bt)w_2(\bt)\right|\le C|w(\bt)|_\tl =C|x_1(\bt) + x_2(\bt)|_\tl.%\sup_{s\le \bt} Ce^A (1+\bt-s)^{-\delta} |w(\bt)|_\tl \le Ce^A |w(\bt)|_\tl =|x_1(\bt) + x_2(\bt)|_\tl.
\end{align*}
Since $|x_1(s)+x_2(s)|_\tl \le |x(s)|_\tl \le \|x\|_\tl$, for any $a \le s\le \bt$ 
\begin{align} \label{lowerbdd}
(2C)^{-1} |x_1(s)+x_2(s)|_\tl \le |x_1(\bt) + x_2(\bt)|_\tl ,
\end{align}
Since $\bt$ was arbitrary and $C$ is not dependent on $\bt$, \eqref{lowerbdd} holds for any $\bt,s \in \mathbb{R}$ with $\bt\ge s\ge a$. In particular, since $|x_1(a)|_\tl=|y_1(a)| >0$, the above estimate shows that $|x_1(t)+x_2(t)|_\tl\ge (2C)^{-1}|y_1(a)|=:m>0 $ for all $t$.

We finally claim that $ \displaystyle\limsup_{t \rightarrow \infty} |x(t)|_\tl = \infty$. Suppse not. Then $ \|x(t)\|_{L^\infty_\tl[\bar a,\infty)}$ is bounded for some $\bar a$. Hence we can choose $s$ so large that for $j=1,2$
$$\int_s^\infty \left|\big(\Phi_j(s,\tau)e^{\int_s^\tau \tl(\eta) \; d\eta}\big) (\E(\tau)x(\tau)e^{\int_a^\tau -\tl(\eta) \; d\eta})_j \right| \; d\tau \le \frac{m}{2}.$$
Using this in the second and the third equations of \eqref{integral4}, by triangular inequality 
\begin{align*}
|w_1(s) + w_2(s)|_\tl \ge |x_1(s)+x_2(s)|_\tl - \frac{m}{2} \ge \frac{m}{2}
% |\Phi_1(s,t)x_1(t) + \Phi_2(s,t)x_2(t)|_\tl \ge |x_1(t) +x_2(t)|_\tl - \frac{m'}{2} \ge \frac{m'}{2}. 
\end{align*}
and from \eqref{eq:decay1} and \eqref{eq:decay2}, $ |w_1(s) + w_2(s)|_\tl \le C (1+t-s)^{-\delta} |x_1(t)+x_2(t)|_\tl$ for any $t\ge s\ge \bar a$. Therefore
\begin{align*}
\frac{m}{2C}(1+t-s)^\delta \le |x_1(t)+x_2(t)|_\tl \quad \text{for any $t\ge s\ge \bar a$}.
\end{align*}
As $\|x(t)\|_{L^\infty_\tl[\bar a,\infty)}$ is bounded, this only can hold if $m=0$ which contradicts.
\end{proof}


\section{Discussion} \label{sec:discuss}
Working with upper triangular matrices offer more flexibility in applications, because not all matrix famiies of matrices allow continuous Jordan factorization, and even if all the blocks do, transforming the matrix to Jordan form may be unnecessary. Returning to the example in Section \ref{sec:intro}, taking the $3\times 3$ matrix as a whole is sufficient if we are only to show the stability of the trivial solution.  Another possibility is that we may only be interested in one block and not the other, hence we may be careless for factorizing the latter.

For the better exposition, a point of view of invariant manifold theory on Levinson's theorem is useful. Levinson's theorem applies to a system $x'(t) = \Lambda(t)x(t)$ for $\Lambda(t)$ diagonal with distinct eigenvalues $\{\lambda_j(t)\}_{j=1}^n$. Let $\mathbf{e}_j$ be the coordinate basis and $< \mathbf{q}_1, \cdots, \mathbf{q}_k>$ denote the vector space spanned by $\mathbf{q}_1 , \cdots, \mathbf{q}_k$. For such a diagonal system, each $< \mathbf{e}_j>$ is one dimensional invariant subspace and 
$$\mathbb{R}^N = < \mathbf{e}_1> \oplus < \mathbf{e}_2> \oplus \cdots \oplus < \mathbf{e}_N>$$
is an invariant splitting of the phase space. Putting $t'=1$ as a dummy equation extends the fixed point $ \mathbf{0}$ of the system to an invariant line $M:=\text{\{$t$-axis\}}$ of the extended system. The splitting becomes that of a tangent bundle along $M$. With this framework, Levinson's theorem can be viewed as the persistence theorem of the splitting.

Let us consider a system $x'(t)=J(t)x(t)$ for a single $N \times N$ Jordan block  $J(t)$, where we first suppose that the shared eigenvalue $\lambda(t)$ has geometric multiplicity $1$. Then the invariant subspace structure for the system is 
$$ < \mathbf{e}_1> ~\hookrightarrow~ < \mathbf{e}_1> \oplus < \mathbf{e}_2>~ \hookrightarrow~ \cdots ~\hookrightarrow~ < \mathbf{e}_1> \oplus \cdots \oplus < \mathbf{e}_N>.$$
The coordinate basis $\mathbf{e}_1$ is the only eigenvector of $J(t)$ and $\mathbf{e}_j$ is an eigenvector of $\Big(J(t)-\lambda(t)\Big)^j$ with $\Big(J(t)-\lambda(t)\Big)\mathbf{e}_j \,\in\, < \mathbf{e}_1> \oplus \cdots \oplus < \mathbf{e}_{j-1}>$. A natural question is to find the sufficient smallness conditions on perturbations to retain this cascading invariant structure. This system has no spectral gap, since the eigenvalue is shared, which suggest this problem is essentially of a single Jordan block. Jordan system fundamental matrices are explicit,i.e., we know that the orbits in $E_j$ can at most grow at rate $t^{j-1}e^{\int_a^t \lambda(\eta)\;d\eta}$. \cite{C65} and \cite{E89} characterized the sufficiency conditions (Equation (6.5) in \cite[p.210]{BL15}) for smallness of perturbation.

Now let $J(t)$ be block diagonal with several Jordan blocks Its hierarchy of invariant subspaces is clear following the previous discussion. Let $E^\alpha_j$ be the $j$-th cascaded subspace of the $\alpha$-th block. Now it makes senses to take the spectral gaps between blocks into account, as well as the perturbation sizes. We see that the underlying theory will be combinatorial. The most complete picture is to keep all those structures persistent, and the problem of finding sufficient conditions has been studied previously \cite[Theorem 6.6, Equation (6.29)]{BL15}. The condition is a combination of spectral gap and perturbation smallness condition. \cite{BL15} used a different approach, confirming the existence of $N$ independent orbits.

From the discussion, it is seen that the multiple Jordan blocks problem consists of two independent problems as follows. 
\begin{enumerate}
\item Problem on a single Jordan block for internal cascaded invariant subspaces $E_j$.
\item Problem on a block diagonal matrix for block-wise invariant subspaces $E^\alpha$.
\end{enumerate}
This paper is on the second problem where a few or all blocks need not have Jordan form. The critical facter is the the availability of estimates $\|\Phi^\alpha(t,s)\|$ and $\|\Phi^\alpha(t,s)^{-1}\|$ for the block fundamental matrices, which is explicit when a block has Jordan form. From Proposition \ref{stability}, estimates are available solely from eigenvalues for upper triangular blocks (fundamental matrices of Jordan block are also upper triangular). 

\section*{Appendices}
For convenience, we include Levinson's theorem \cite{L48} in the following form.
\begin{theorem}[Levinson's theorem] \label{thm:CL} Let $x(t)\in \mathbb{R}^N$ and $x'(t) = \big(\Lambda(t) + \mathcal{E}(t)\big)x$, where $\Lambda(t)$ is a diagonal matrix with diagonal entries $\lambda_j(t)$, $j=1,\cdots,N$ bounded and $\mathcal{E}(t)$ is a matrix such that $\|\mathcal{E}\|$ integrable, i.e., $\int_{a_0}^\infty \|\mathcal{E}(t)\| dt < \infty$ for some $a_0$.
Fix an index $k$. Suppose we can find some constant $A$ so that either of the following two membership conditions holds for every $i$.

$i \in I_1$ if
\begin{align}
&\int_{a_0}^\infty Re(\lambda_k(s) -\lambda_i(s))\; ds \rightarrow \infty \quad \text{as $t \rightarrow \infty$ for some $a_0$},\label{eq:I1cond1}\\
&\int_{t_1}^{t_2} Re(\lambda_k(s) -\lambda_i(s))\; ds > -A, \quad \text{whenever $t_2\ge t_1\ge 0$} \label{eq:I1cond2}
\end{align}
and $i \in I_2$ if
\begin{align}
&\int_{t_1}^{t_2} Re(\lambda_k(s) -\lambda_i(s))\; ds < A, \quad \text{whenever $t_2\ge t_1\ge 0$}. \label{eq:I2cond}
\end{align}
Then there is an orbit $\varphi_k(t)$ $t\ge a$ for some $a$ such that,
\begin{equation}
\lim_{t \rightarrow \infty} \varphi_k(t) \exp\left(-\int_{a}^t \lambda_k(s)\; ds\right) = \hat{k}, \quad \text{where $\hat{k}$ is the $k$-th coordinate basis of $\mathbb{R}^d$.}
\end{equation}
\end{theorem}

The following derivation is a small improvement of \cite[Theorem 1]{L19} with abstract assumptions. The proof is identical.
\begin{theorem} \label{result0} Suppose that $U(t) = diag(B_0(t),B_1(t))$ with $B_0(t)$ and $B_1(t)$ of dimensions $N_0\times N_0$ and $N_1 \times N_1$ respectively. Let $\{\Phi_{B_0}(t,\tau)\}_{t,\tau \in \mathbb{R}}$ and $\{\Phi_{B_1}(t,\tau)\}_{t,\tau \in \mathbb{R}}$ be their fundamental matrices respectively. We assume the followings.
\begin{enumerate}
\item $\int_{a_0}^\infty \|\E(t)\| \; < \infty$ for some $a_0$.
\item There is a real-valued function $\theta$ and a constant $C>0$ such that
\begin{enumerate}
\item for any $t\ge \tau \ge a_0$
$$\|\Phi_{B_0}(t,\tau) e^{-\int_{t}^{\tau} \theta(\eta)\; d\eta}\| \le C,$$
and for any $\tau \ge a_0$ \begin{equation}\label{eq:decay} \lim_{t \rightarrow \infty}\|\Phi_0(t,\tau)e^{-\int_{\tau}^t \theta(\eta)\;d\eta}\| = 0,\end{equation}
\item for any $t\ge \tau \ge a_0$
$$\|\Phi_{B_1}(\tau,t) e^{\int_{\tau}^{t} \theta(\eta)\; d\eta}\| \le C$$
\end{enumerate}
\end{enumerate}
Then there is a constant $a$ and an $N_0$-dimensional subspace $E_0$ of $ \mathbb{R}^N$ such that $x(a)\in E_0$ implies $\displaystyle \lim_{t \rightarrow \infty}|x(t)|_\theta =0$. %One such an $E_0 = P_0\mathbb{R}^N$.
\end{theorem}
\begin{proof}
Let $y(a)$ be a vector such that $P_0y(a) = y(a)$ and $y(t) = \Phi(t,a)y(a)$. We look for a solution of the following integral equation.
\begin{equation*} \label{eq:integral}
x(t) = y(t) + \int_a^t P_0\Phi(t,\tau)\E(\tau)x(\tau) \;d\tau - \int_t^\infty P_1\Phi(t,\tau)\E(\tau)x(\tau) \; d\tau.
\end{equation*}
If exists, that $x(t)$ solves \eqref{eq:2} follows from that each column of $\Phi(t,\tau)$ solves \eqref{eq:1}. In particular, we have $P_0x(a) = y(a)$.
Suppose such an $x(t)$ exists. Multiplying both sides by $e^{-\int_a^t \theta(\eta) \;d\eta}$ and using block matrices calculus, 
\begin{equation} \label{eq:integral2}
\begin{aligned}
x(t)e^{-\int_a^t \theta(\eta) \;d\eta} &= y(t)e^{-\int_a^t \theta(\eta) \;d\eta} + \int_a^t \big(\Phi_0(t,\tau)e^{-\int_\tau^t \theta(\eta) \;d\eta}\big)\big(\E(\tau) x(\tau)e^{-\int_a^\tau \theta(\eta) \;d\eta}\big)_0 \;d\tau \\
&- \int_t^\infty \big(\Phi_1(t,\tau)e^{\int_t^\tau \theta(\eta) \;d\eta}\big) \big(\E x(\tau)e^{-\int_a^\tau \theta(\eta) \;d\eta}\big)_1 \; d\tau.
\end{aligned}
\end{equation}
Since $\|\E(t)\|$ is integrable, we can choose $a$ so that $C\int_a^\infty \|\E(\tau)\| \;d\tau < \frac{1}{2}$. Then, we have
\begin{align*}
|x(t)-y(t)|_{\theta} &\le \int_a^t \|\Phi_0(t,\tau)e^{-\int_\tau^t \theta(\eta)}\;d\eta\| \:\|\E(\tau)\| \: |x(\tau)|_\theta \; d\tau \\
&+ \int_t^\infty \|\Phi_1(t,\tau)e^{\int_\tau^t \theta(\eta)}\;d\eta\| \: \|\E(\tau)\| \:|x(\tau)|_\theta \; d\tau\\
&\le \frac{1}{2}\|x\|_{\theta}.
\end{align*}

Let $y$ be fixed and $S^y$ be the operator on $L^\infty_{\theta}([a,\infty))$ that maps $x \in L^\infty_{\theta}([a,\infty))$ to the function the right-hand-side of \eqref{eq:integral} defines. The previous estimate shows that $\|S^y x\|_\theta \le \|y\|_\theta + \frac{1}{2} \|x\|_\theta < \infty$ and thus $S^y x \in L^\infty_{\theta}([a,\infty))$, hence the solution of the integral equation is the fixed point of the operator. The same estimate shows that $\|S^y x - S^y \bar{x}\|_\theta \le \frac{1}{2} \|x-\bar{x}\|_\theta$. By the contraction mapping principle, there is a unique fixed point. 

It is clear that $\|x\|_\theta \le 2\|y\|_\theta$. The map $y(a) \mapsto x(a)$, where the function $x$ is the unique fixed point of $S^y$, is a linear map from $E_0$ to its image. As $P_0x(a)=y(a)$, the linear map is injective and has rank $N_0$. $E$ is then the range space.


We know that $|y(t)|_\theta \rightarrow 0$ as $t \rightarrow \infty$. It remains to show that $|x(t)|_\theta \rightarrow 0$ as $t \rightarrow \infty$ as well. We show that $|x(t)-y(t)|_\theta \rightarrow 0$. Let the first integral in \eqref{eq:integral2} be $I_1$ and the second be $I_2$. $I_2$ converges to $0$ since
$$\lim_{t \rightarrow\infty}\int_t^\infty \|\Phi_1(t,\tau)e^{\int_\tau^t \theta(\eta)}\;d\eta\| \: \|\E(\tau)\| \:|x(\tau)|_\theta \; d\tau\le 2C\|y\|_\theta \lim_{t \rightarrow \infty}\int_t^\infty \|\E(\tau)\| \;d\tau = 0.$$
For $I_1$, we divide the integral into 
$$\left(\int_a^{t_1} + \int_{t_1}^t \right) \left\{\big(\Phi_0(t,\tau)e^{-\int_\tau^t \theta(\eta) \;d\eta}\big)P_0\big(\E(\tau) x(\tau)e^{-\int_a^\tau \theta(\eta) \;d\eta}\big) \;d\tau\right\}.$$
for some $t_1 \le t$. For any $\epsilon>0$, we can choose $t$ and $t_1$ so large enough, while retaining $t\ge t_1$, that $I_1 \le \epsilon$. With the same reasoning used for $I_2$, we can choose $t_1$ so large that the integral over $[t_1, t]$ is smaller than $ \frac{\epsilon}{2}$. We can express the integral in interval $[a,t_1]$ as
\begin{align*}
% &\int_a^{t_1} \big(\Phi_0(t,\tau)e^{-\int_\tau^t \theta(\eta) \;d\eta}\big)\big(\E(\tau) x(\tau)e^{-\int_a^\tau \theta(\eta) \;d\eta}\big)_0 \;d\tau \\
\big(\Phi_0(t,t_1)e^{-\int_{t_1}^t \theta(\eta)\;d\eta}\big) \int_a^{t_1} \big(\Phi_0(t_1,\tau)e^{-\int_\tau^{t_1} \theta(\eta) \;d\eta}\big)P_0\big(\E(\tau) x(\tau)e^{-\int_a^\tau \theta(\eta) \;d\eta}\big) \;d\tau.
\end{align*}
From \eqref{eq:decay}, $\|\Phi_0(t,t_1)e^{-\int_{t_1}^t \theta(\eta)\;d\eta}\| \rightarrow 0$ as $t \rightarrow \infty$, and the integral in $[a,t_1]$ must be finite. Therefore we can choose $t$ so large that the above is smaller than $ \frac{\epsilon}{2}$.

\end{proof}



\begin{thebibliography}{999}
% Reference 1

%\bibitem[Bodine and Lutz(2007)]{BL07}
%Bodine, S and Lutz, D. A. On asymptotic equivalence of perturbed linear systems of differential and difference equations. {\em J. Math. Anal. Appl.} {\bf 2007}, {\em 326}, 1174--1189.

\bibitem[Bodine and Lutz(2010)]{BL10}
Bodine, S and Lutz, D. A. Asymptotic integration under weak dichotomies. {\em Rocky Mt. J. Math.} {\bf 2010}, {\em 40}, 51--75.

% \bibitem[Bodine and Lutz(2011)]{BL11}
% Bodine, S and Lutz, D. A. Asymptotic integration of nonoscillatory differential equations: a unified approach. {\em J. Dyn. Control. Syst.} {\bf 2011}, {\em 17}, 329--358.


\bibitem[Bodine and Lutz(2015)]{BL15}
Bodine, S and Lutz, D. A. {\em Asymptotic Integration of Differential and Difference Equations}; Lecture Notes in Mathematics Vol 2129, Springer International Publishing, 2015.

\bibitem[Chiba and Kimura(1970)]{CK70}
Chiba, K., and Kimura, T. On the asymptotic behavior of solutions of a system of linear ordinary differential equations. {\em Rikkyo Daigaku sugaku zasshi} {\bf 1970}, {\em 18}, 61--80.

\bibitem[Coddington and Levinson(1955)]{CL55}
Coddington, E.A., Levinson, N. {\em Theory of ordinary differential equations}; McGraw-Hill Inc., New York, 1955; pp. 32--58.

\bibitem[Coppel(1965)]{C65}
Coppel, W.A. {\em Stability and Asymptotic Behavior of Differential Equations}; Heath mathematical monographs, D. C. Heath, 1965.


\bibitem[Devinatz and Kaplan(1972)]{DK72}
Devinatz, A. and Kaplan, J. L. Asymptotic estimates for solutions of linear systems of ordinary differential equations having multiple characteristic roots. {\em Indiana Univ. Math. J.} {bf 1972}, {\em 22}, 355--366.

\bibitem[Eastham(1989)]{E89}
Eastham, M.S.P. {\em The Asymptotic Solution of Linear Differential Systems: Application of the Levinson Theorem}; London Mathematical Society Monographs New Series Vol 4, Oxford University Press, Oxford, 1989.

\bibitem[Lee(2019)]{L19}
Lee, M. Asymptotic stability of non-autonomous systems and a generalization of Levinson's theorem. {\em Mathematics} {\bf 2019}, {\em 7}, 1213.

\bibitem[Levinson(1948)]{L48}
Levinson, N. The asymptotic nature of solutions of linear systems of differential equations. {\em Duke Math. J.} {\bf 1948}, {\em 15}, 111--126.

\bibitem[Markus and Yamabe(1960)]{MY60}
Markus, L. and Yamabe, H. Global stability criteria for differential systems. {\em Osaka Math. J.} {\bf 1960}, {\em 12}, 305--317.


\end{thebibliography}


\end{document}




